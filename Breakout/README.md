# Breakout

The objective of the breakout game is to hit as many blocks as possible without missing the ball. The action space in the GYM implementation has four actions, do nothing, right, left and fire a new ball. You can see the different agents playing in data/results.

## Breakout_SL

This code contains a random agent playing the game and we can notice that it's extremely bad. This is important to compare it later with other agents. 

We have also an AI agent which calculates the paddle and ball position and depending on this information moves. The functions used to calculate the ball and paddle x coordinate are entirely made by ourselves. This agent performs surprisingly good. The objective of this AI agent was to check the implementation of the functions calculate_ball() and calculate_paddle(). 

Finally we have the supervised learning agent that uses the information generated by the AI agent playing to train. The input size for this agent is an image and it uses a convolutional neural network. The objective of the game was to see if it was possible to use a convolutional neural network to learn the game basics and the result was successful. We could have trained longer or add more data but our objective was just to make a first approach and then move to reinforcement learning. You will observe that the image that we pass to our agent is not the actual frame image but a concatenation of three images that includes the actual frame and the two previous frames to add a sense of ball motion. Otherwise the agent will not know the ball direction and where he is moving.

## Breakout_RL
In this file there's the implementation and test of the Reinforcement Learning agent to play Breakout.

The code structure is very similar to the Lunar Lander example. In fact, we adapted this code from the LunarLander file. Some changes have been made to make it work.

First of all we noticed that with images we were not able to achieve significant result due the training time required. The original observation space (information for the agent) was an image of (210,160,3) and we changed to a vector of two dimensions that contains the ball and paddle position. Also we have changed the reward which we calculated in terms of the distance between the paddle and the ball. This changes reduce the training time and makes it more stable.

We have replaced the LunarLander agent deep neural network for a Dueling DQN approach. The code used can be found in https://github.com/RMiftakhov/LunarLander-v2-drlnd/blob/master/model.py, we decided to use this net because it gave better and more stable results.

